# AI Image Detector

A deep learning project for detecting AI-generated images using ResNeXt architectures and Transformer-based approaches. This project addresses the growing need to distinguish synthetic imagery from real photographs in the era of advanced generative models.

## Overview

The rapid advancement of generative AI models (like Stable Diffusion, DALL-E 3, Midjourney) has made it increasingly difficult to differentiate between real and fake images. This project implements and evaluates robust deep learning classifiers to solve this binary classification problem.

We explore multiple architectures:
- **ResNeXt-50 / ResNeXt-101**: Strong convolutional backbones for feature extraction.
- **Hybrid Transformers**: Combining CNN backbones with Transformer encoders to capture global dependencies.
- **Ensemble Methods**: Weighted averaging of multiple models to improve generalization and robustness.

## Dataset

This project utilizes a diverse set of Real vs. AI datasets to ensure robust evaluation:
- **DRAGON**: A large-scale dataset of realistic images generated by diffusion models (SDXL, DALL-E 3, etc.).
- **ArtiFact**: A curated collection of real and fake images from various sources (Flickr, COCO vs. GANs/Diffusion).
- **DeepArmocromia**: A specialized dataset for seasonal color analysis, used for testing domain-specific robustness.
- **ImageNet-1k**: Used as a source of high-quality real images.

## Methodology

### 1. ResNeXt Backbone
We leverage **ResNeXt-50 (32x4d)** and **ResNeXt-101 (32x8d)** pre-trained on ImageNet. Code modifications allow for:
- Fine-tuning specific layers.
- Freezing/unfreezing strategies to adapt to the new domain.
- Custom classification heads (FC layers + Dropout).

### 2. Transformer Integration
To capture long-range dependencies that CNNs might miss, we experimented with a hybrid architecture:
- **Feature Extraction**: ResNeXt backbone.
- **Projection**: Mapping CNN features to a sequence.
- **Transformer Encoder**: Processing the sequence to learn global context.
- **MLP Head**: Final classification (Real vs. AI).

## Results

Our models achieve high accuracy across various test sets. Key findings include:
- **ResNeXt-101** generally outperforms ResNeXt-50 due to deeper feature extraction capability.
- **Ensemble methods** (weighted averaging of predictions) consistently provide the most stable and accurate results, often exceeding 95% accuracy on benchmark test sets.
- The models demonstrate robustness even on difficult examples (e.g., highly realistic human faces or complex scenes).

## Project Structure

```
├── configs/        # Configuration scripts for model training/inference
├── data/           # Dataset storage (splits, test images)
├── docs/           # Detailed documentation and usage guides
├── models/         # Model storage
│   └── checkpoints/ # Saved .pt model files
├── runs/           # Training logs
├── scripts/        # Utility and testing scripts
├── src/            # Source code
│   ├── models/     # Model architecture definitions
│   ├── utils/      # Data loading, file handling utilities
│   └── pipeline.py # Core training and testing logic
├── tests/          # Unit tests
├── main.py         # CLI entry point
└── requirements.txt # Project dependencies
```

## Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/AI_Image_Detector.git
    cd AI_Image_Detector
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Usage

This project uses a CLI entry point `main.py` driven by configuration files located in `configs/`.

### Quick Start
To run testing on an existing model:

```bash
python main.py \
  --configs configs/mod_resNeXt_ft.py \
  --path_test data/splits/test.csv \
  --path_model models/checkpoints/resnext.pt
```

For a comprehensive guide on all available commands and configurations (Training, Testing, Inference), please refer to [docs/USAGE.md](docs/USAGE.md).

## Credits

**Final Project - Computer Vision**
Based on state-of-the-art research in synthetic image detection.

**References:**
- *ResNeXt*: Xie, S., et al. (2017). "Aggregated Residual Transformations for Deep Neural Networks."
- *DRAGON Dataset*: Bertazzini, G., et al. (2025).
- *ImageNet*: Deng, J., et al. (2009).